{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn sentence-transformers plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261a77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing topic_labeling_pilot_Brazil.xlsx (Annotator: Brazil)...\n",
      "  Found sheets: ['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8 ', 'Topic_9', 'Topic_10', 'Topic_11', 'Topic_12', 'Topic_13', 'Topic_14', 'Topic_15', 'Topic_16', 'Topic_17', 'Topic_18', 'Topic_19', 'Topic_20']\n",
      "  Successfully processed 21 topic sheets\n",
      "\n",
      "Processing topic_labeling_pilot_China_Xiawei.xlsx (Annotator: China_Xiawei)...\n",
      "  Found sheets: ['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8', 'Topic_9', 'Topic_10', 'Topic_11', 'Topic_12', 'Topic_13', 'Topic_14', 'Topic_15', 'Topic_16', 'Topic_17', 'Topic_18', 'Topic_19', 'Topic_20']\n",
      "  Successfully processed 21 topic sheets\n",
      "\n",
      "Processing topic_labeling_pilot_China_Xinyi.xlsx (Annotator: China_Xinyi)...\n",
      "  Found sheets: ['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8', 'Topic_9', 'Topic_10', 'Topic_11', 'Topic_13', 'Topic_12', 'Topic_14', 'Topic_15', 'Topic_16', 'Topic_17', 'Topic_18', 'Topic_19', 'Topic_20']\n",
      "  Successfully processed 21 topic sheets\n",
      "\n",
      "Processing topic_labeling_pilot_Poland.xlsx (Annotator: Poland)...\n",
      "  Found sheets: ['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8', 'Topic_9', 'Topic_10', 'Topic_11', 'Topic_12', 'Topic_13', 'Topic_14', 'Topic_15', 'Topic_16', 'Topic_17', 'Topic_18', 'Topic_19', 'Topic_20']\n",
      "  Successfully processed 21 topic sheets\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 4 files\n",
      "Failed to process: 0 files\n",
      "==================================================\n",
      "\n",
      "Data check per topic:\n",
      "  Topic_0: 4 annotators\n",
      "  Topic_1: 4 annotators\n",
      "  Topic_10: 4 annotators\n",
      "  Topic_11: 4 annotators\n",
      "  Topic_12: 4 annotators\n",
      "  Topic_13: 4 annotators\n",
      "  Topic_14: 4 annotators\n",
      "  Topic_15: 4 annotators\n",
      "  Topic_16: 4 annotators\n",
      "  Topic_17: 4 annotators\n",
      "  Topic_18: 4 annotators\n",
      "  Topic_19: 4 annotators\n",
      "  Topic_2: 4 annotators\n",
      "  Topic_20: 4 annotators\n",
      "  Topic_3: 4 annotators\n",
      "  Topic_4: 4 annotators\n",
      "  Topic_5: 4 annotators\n",
      "  Topic_6: 4 annotators\n",
      "  Topic_7: 4 annotators\n",
      "  Topic_8: 3 annotators\n",
      "  Topic_8 : 1 annotators\n",
      "  Topic_9: 4 annotators\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "At least one sheet must be visible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data was extracted. Please check file paths.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Write to Excel with all topics as separate sheets\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mExcelWriter(output_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# Sort topics properly (Topic_0, Topic_1, ..., Topic_20)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         sorted_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(all_topics_data\u001b[38;5;241m.\u001b[39mkeys(), \n\u001b[1;32m    121\u001b[0m                               key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m sorted_topics:\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;66;03m# Combine all rows for this topic (one from each annotator)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/pandas/io/excel/_base.py:1353\u001b[0m, in \u001b[0;36mExcelWriter.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1349\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1350\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1351\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1352\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/pandas/io/excel/_base.py:1357\u001b[0m, in \u001b[0;36mExcelWriter.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"synonym for save, to make it more file-like\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/pandas/io/excel/_openpyxl.py:110\u001b[0m, in \u001b[0;36mOpenpyxlWriter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    Save workbook to disk.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle, mmap\u001b[38;5;241m.\u001b[39mmmap):\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;66;03m# truncate file to the written content\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mtruncate()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/workbook/workbook.py:386\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[0;32m--> 386\u001b[0m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/writer/excel.py:294\u001b[0m, in \u001b[0;36msave_workbook\u001b[0;34m(workbook, filename)\u001b[0m\n\u001b[1;32m    292\u001b[0m workbook\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mmodified \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(tz\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39mreplace(tzinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n\u001b[0;32m--> 294\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/writer/excel.py:275\u001b[0m, in \u001b[0;36mExcelWriter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/writer/excel.py:89\u001b[0m, in \u001b[0;36mExcelWriter.write_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m writer \u001b[38;5;241m=\u001b[39m WorkbookWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkbook)\n\u001b[1;32m     88\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_ROOT_RELS, writer\u001b[38;5;241m.\u001b[39mwrite_root_rels())\n\u001b[0;32m---> 89\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_WORKBOOK, \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m archive\u001b[38;5;241m.\u001b[39mwritestr(ARC_WORKBOOK_RELS, writer\u001b[38;5;241m.\u001b[39mwrite_rels())\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_vba()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/workbook/_writer.py:150\u001b[0m, in \u001b[0;36mWorkbookWriter.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_names()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_pivots()\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_views\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_refs()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tostring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpackage\u001b[38;5;241m.\u001b[39mto_tree())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/workbook/_writer.py:137\u001b[0m, in \u001b[0;36mWorkbookWriter.write_views\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_views\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     active \u001b[38;5;241m=\u001b[39m \u001b[43mget_active_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb\u001b[38;5;241m.\u001b[39mviews:\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb\u001b[38;5;241m.\u001b[39mviews[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactiveTab \u001b[38;5;241m=\u001b[39m active\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/openpyxl/workbook/_writer.py:35\u001b[0m, in \u001b[0;36mget_active_sheet\u001b[0;34m(wb)\u001b[0m\n\u001b[1;32m     33\u001b[0m visible_sheets \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, sheet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wb\u001b[38;5;241m.\u001b[39m_sheets) \u001b[38;5;28;01mif\u001b[39;00m sheet\u001b[38;5;241m.\u001b[39msheet_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisible\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m visible_sheets:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one sheet must be visible\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m idx \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39m_active_sheet_index\n\u001b[1;32m     38\u001b[0m sheet \u001b[38;5;241m=\u001b[39m wb\u001b[38;5;241m.\u001b[39mactive\n",
      "\u001b[0;31mIndexError\u001b[0m: At least one sheet must be visible"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path and file names\n",
    "base_path = \"/Users/vinitavader/Desktop/WJC/Vinita/annotation_task_data\"\n",
    "files = [\n",
    "    \"topic_labeling_pilot_Brazil.xlsx\",\n",
    "    \"topic_labeling_pilot_China_Xiawei.xlsx\",\n",
    "    \"topic_labeling_pilot_China_Xinyi.xlsx\",\n",
    "    \"topic_labeling_pilot_Poland.xlsx\"\n",
    "]\n",
    "\n",
    "# Define the columns we want to extract\n",
    "target_columns = [\n",
    "    \"Label_1\", \"Label_1_Reason\", \n",
    "    \"Label_2\", \"Label_2_Reason\", \n",
    "    \"Label_3\", \"Label_3_Reason\"\n",
    "]\n",
    "\n",
    "# Dictionary to store data for each topic\n",
    "all_topics_data = {}\n",
    "\n",
    "# Track which files were successfully processed\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = os.path.join(base_path, file)\n",
    "    \n",
    "    # Extract annotator name FROM THE FILENAME ONLY\n",
    "    if \"Brazil\" in file:\n",
    "        annotator = \"Brazil\"\n",
    "    elif \"China_Xiawei\" in file:\n",
    "        annotator = \"China_Xiawei\"\n",
    "    elif \"China_Xinyi\" in file:\n",
    "        annotator = \"China_Xinyi\"\n",
    "    elif \"Poland\" in file:\n",
    "        annotator = \"Poland\"\n",
    "    \n",
    "    print(f\"\\nProcessing {file} (Annotator: {annotator})...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"  ERROR: File does not exist at {file_path}\")\n",
    "            failed_files.append(file)\n",
    "            continue\n",
    "            \n",
    "        # Read all sheets from the Excel file\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        print(f\"  Found sheets: {excel_file.sheet_names}\")\n",
    "        \n",
    "        topics_found = 0\n",
    "        \n",
    "        # Process each topic sheet\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            if sheet_name.startswith(\"Topic_\"):\n",
    "                # Read the sheet\n",
    "                df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                \n",
    "                # Check if dataframe is empty\n",
    "                if df.empty:\n",
    "                    print(f\"  WARNING: {sheet_name} is empty!\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if all required columns exist\n",
    "                missing_cols = [col for col in target_columns if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    print(f\"  Warning: Missing columns in {sheet_name}: {missing_cols}\")\n",
    "                    # Create empty columns for missing ones\n",
    "                    for col in missing_cols:\n",
    "                        df[col] = \"\"\n",
    "                \n",
    "                # Extract the required columns (first row only)\n",
    "                row_data = df[target_columns].iloc[0:1].copy()\n",
    "                \n",
    "                # Add the annotator column (from filename, NOT from inside the file)\n",
    "                row_data.insert(0, 'Annotator', annotator)\n",
    "                \n",
    "                # Add to our dictionary\n",
    "                if sheet_name not in all_topics_data:\n",
    "                    all_topics_data[sheet_name] = []\n",
    "                all_topics_data[sheet_name].append(row_data)\n",
    "                topics_found += 1\n",
    "        \n",
    "        print(f\"  Successfully processed {topics_found} topic sheets\")\n",
    "        processed_files.append(file)\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: File not found - {file_path}\")\n",
    "        failed_files.append(file)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {file}: {type(e).__name__}: {str(e)}\")\n",
    "        failed_files.append(file)\n",
    "        continue\n",
    "\n",
    "# Summary of processing\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"PROCESSING SUMMARY:\")\n",
    "print(f\"Successfully processed: {len(processed_files)} files\")\n",
    "print(f\"Failed to process: {len(failed_files)} files\")\n",
    "if failed_files:\n",
    "    print(f\"Failed files: {failed_files}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Check data consistency\n",
    "if all_topics_data:\n",
    "    print(\"Data check per topic:\")\n",
    "    for topic in sorted(all_topics_data.keys()):\n",
    "        print(f\"  {topic}: {len(all_topics_data[topic])} annotators\")\n",
    "\n",
    "# Create the output Excel file\n",
    "output_path = os.path.join(base_path, \"consolidated_topic_labels.xlsx\")\n",
    "\n",
    "if not all_topics_data:\n",
    "    print(\"No data was extracted. Please check file paths.\")\n",
    "else:\n",
    "    # Write to Excel with all topics as separate sheets\n",
    "    # with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    #     # Sort topics properly (Topic_0, Topic_1, ..., Topic_20)\n",
    "    #     sorted_topics = sorted(all_topics_data.keys(), \n",
    "    #                           key=lambda x: int(x.split('_')[1]))\n",
    "        \n",
    "    #     for topic in sorted_topics:\n",
    "    #         # Combine all rows for this topic (one from each annotator)\n",
    "    #         topic_df = pd.concat(all_topics_data[topic], ignore_index=True)\n",
    "            \n",
    "            # Write to Excel\n",
    "            #topic_df.to_excel(writer, sheet_name=topic, index=False)\n",
    "            \n",
    "            #print(f\"Written {topic} with {len(topic_df)} rows\")\n",
    "    \n",
    "    #print(f\"\\nConsolidated Excel file created at: {output_path}\")\n",
    "    \n",
    "    # Also create a summary with all data in one sheet\n",
    "    summary_data = []\n",
    "    for topic in sorted_topics:\n",
    "        topic_df = pd.concat(all_topics_data[topic], ignore_index=True)\n",
    "        topic_df.insert(0, 'Topic', topic)\n",
    "        summary_data.append(topic_df)\n",
    "    \n",
    "    summary_df = pd.concat(summary_data, ignore_index=True)\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = os.path.join(base_path, \"consolidated_topic_labels_summary.xlsx\")\n",
    "    #summary_df.to_excel(summary_path, sheet_name='All_Topics', index=False)\n",
    "    \n",
    "    #print(f\"Summary file created at: {summary_path}\")\n",
    "    \n",
    "    # Show which annotators are in the final data\n",
    "    #print(f\"\\nAnnotators in final data: {summary_df['Annotator'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8269325d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Label_1</th>\n",
       "      <th>Label_1_Reason</th>\n",
       "      <th>Label_2</th>\n",
       "      <th>Label_2_Reason</th>\n",
       "      <th>Label_3</th>\n",
       "      <th>Label_3_Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic_0</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Workplace and Ethical Decisions</td>\n",
       "      <td>Decisions related to the life cycle, the trans...</td>\n",
       "      <td>Workplace and Ethical Decisions</td>\n",
       "      <td>The activation of a moral judgment in response...</td>\n",
       "      <td>Workplace and Ethical Decisions</td>\n",
       "      <td>In the face of institutional overload, a quiet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic_0</td>\n",
       "      <td>China_Xiawei</td>\n",
       "      <td>Career crisis</td>\n",
       "      <td>All answers described crisises in career devel...</td>\n",
       "      <td>Conflicts</td>\n",
       "      <td>All answers described conflicts either betewee...</td>\n",
       "      <td>Career development</td>\n",
       "      <td>All these generally are related to issues in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic_0</td>\n",
       "      <td>China_Xinyi</td>\n",
       "      <td>Work stress</td>\n",
       "      <td>All respondents describe emotional of physical...</td>\n",
       "      <td>Future life planning</td>\n",
       "      <td>The responses involves participants' considera...</td>\n",
       "      <td>Work-life balance</td>\n",
       "      <td>All responses show they try to strike a balanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic_0</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Contributing</td>\n",
       "      <td>giving value to what one can give as a part of...</td>\n",
       "      <td>work conflicts</td>\n",
       "      <td>pointing out towards personal difficulties wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic_1</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Personal Beliefs</td>\n",
       "      <td>A decision shaped by personal beliefs about fa...</td>\n",
       "      <td>Personal Beliefs</td>\n",
       "      <td>A decision guided by beliefs in technical prog...</td>\n",
       "      <td>Personal Beliefs</td>\n",
       "      <td>A decision grounded in relational values, fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Topic_19</td>\n",
       "      <td>Poland</td>\n",
       "      <td>health problems</td>\n",
       "      <td>although strongly varying in specifics, all ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Topic_20</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Motherhood Challenges</td>\n",
       "      <td>The decision to abandon alcoholism and life on...</td>\n",
       "      <td>Motherhood Challenges</td>\n",
       "      <td>Decisions around trying to conceive and enduri...</td>\n",
       "      <td>Motherhood Challenges</td>\n",
       "      <td>The decision to terminate a new pregnancy to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Topic_20</td>\n",
       "      <td>China_Xiawei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Topic_20</td>\n",
       "      <td>China_Xinyi</td>\n",
       "      <td>Value on next generation</td>\n",
       "      <td>All responses make decisions that were deeply ...</td>\n",
       "      <td>Negative emotions</td>\n",
       "      <td>Each response mention the struggle and anxiety...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Topic_20</td>\n",
       "      <td>Poland</td>\n",
       "      <td>pregnancy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic     Annotator                          Label_1  \\\n",
       "0    Topic_0        Brazil  Workplace and Ethical Decisions   \n",
       "1    Topic_0  China_Xiawei                    Career crisis   \n",
       "2    Topic_0   China_Xinyi                      Work stress   \n",
       "3    Topic_0        Poland                    Contributing    \n",
       "4    Topic_1        Brazil                 Personal Beliefs   \n",
       "..       ...           ...                              ...   \n",
       "79  Topic_19        Poland                  health problems   \n",
       "80  Topic_20        Brazil            Motherhood Challenges   \n",
       "81  Topic_20  China_Xiawei                              NaN   \n",
       "82  Topic_20   China_Xinyi         Value on next generation   \n",
       "83  Topic_20        Poland                        pregnancy   \n",
       "\n",
       "                                       Label_1_Reason  \\\n",
       "0   Decisions related to the life cycle, the trans...   \n",
       "1   All answers described crisises in career devel...   \n",
       "2   All respondents describe emotional of physical...   \n",
       "3   giving value to what one can give as a part of...   \n",
       "4   A decision shaped by personal beliefs about fa...   \n",
       "..                                                ...   \n",
       "79  although strongly varying in specifics, all ca...   \n",
       "80  The decision to abandon alcoholism and life on...   \n",
       "81                                                NaN   \n",
       "82  All responses make decisions that were deeply ...   \n",
       "83                                                NaN   \n",
       "\n",
       "                            Label_2  \\\n",
       "0   Workplace and Ethical Decisions   \n",
       "1                         Conflicts   \n",
       "2              Future life planning   \n",
       "3                    work conflicts   \n",
       "4                  Personal Beliefs   \n",
       "..                              ...   \n",
       "79                              NaN   \n",
       "80            Motherhood Challenges   \n",
       "81                              NaN   \n",
       "82                Negative emotions   \n",
       "83                              NaN   \n",
       "\n",
       "                                       Label_2_Reason  \\\n",
       "0   The activation of a moral judgment in response...   \n",
       "1   All answers described conflicts either betewee...   \n",
       "2   The responses involves participants' considera...   \n",
       "3   pointing out towards personal difficulties wit...   \n",
       "4   A decision guided by beliefs in technical prog...   \n",
       "..                                                ...   \n",
       "79                                                NaN   \n",
       "80  Decisions around trying to conceive and enduri...   \n",
       "81                                                NaN   \n",
       "82  Each response mention the struggle and anxiety...   \n",
       "83                                                NaN   \n",
       "\n",
       "                            Label_3  \\\n",
       "0   Workplace and Ethical Decisions   \n",
       "1                Career development   \n",
       "2                 Work-life balance   \n",
       "3                               NaN   \n",
       "4                  Personal Beliefs   \n",
       "..                              ...   \n",
       "79                              NaN   \n",
       "80            Motherhood Challenges   \n",
       "81                              NaN   \n",
       "82                              NaN   \n",
       "83                              NaN   \n",
       "\n",
       "                                       Label_3_Reason  \n",
       "0   In the face of institutional overload, a quiet...  \n",
       "1   All these generally are related to issues in c...  \n",
       "2   All responses show they try to strike a balanc...  \n",
       "3                                                 NaN  \n",
       "4   A decision grounded in relational values, fami...  \n",
       "..                                                ...  \n",
       "79                                                NaN  \n",
       "80  The decision to terminate a new pregnancy to p...  \n",
       "81                                                NaN  \n",
       "82                                                NaN  \n",
       "83                                                NaN  \n",
       "\n",
       "[84 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820fce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinitavader/.pyenv/versions/3.10.17/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66ff37",
   "metadata": {},
   "source": [
    "## Embeddings for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Sentence Embeddings with BERT (Most Accurate for Semantic Similarity)\n",
    "def analyze_semantic_similarity_bert(df):\n",
    "    # Load pre-trained sentence transformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for topic in df['Topic'].unique():\n",
    "        topic_data = df[df['Topic'] == topic]\n",
    "        \n",
    "        # Collect all labels for this topic\n",
    "        labels = []\n",
    "        label_info = []\n",
    "        \n",
    "        for idx, row in topic_data.iterrows():\n",
    "            for label_col in ['Label_1', 'Label_2', 'Label_3']:\n",
    "                if pd.notna(row[label_col]) and row[label_col] != '':\n",
    "                    labels.append(row[label_col])\n",
    "                    label_info.append({\n",
    "                        'topic': topic,\n",
    "                        'annotator': row['Annotator'],\n",
    "                        'label_type': label_col,\n",
    "                        'label': row[label_col]\n",
    "                    })\n",
    "        \n",
    "        if len(labels) > 1:\n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(labels)\n",
    "            \n",
    "            # Calculate similarity matrix\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Store results\n",
    "            for i in range(len(labels)):\n",
    "                for j in range(i+1, len(labels)):\n",
    "                    results.append({\n",
    "                        'topic': topic,\n",
    "                        'label1': labels[i],\n",
    "                        'annotator1': label_info[i]['annotator'],\n",
    "                        'label_type1': label_info[i]['label_type'],\n",
    "                        'label2': labels[j],\n",
    "                        'annotator2': label_info[j]['annotator'],\n",
    "                        'label_type2': label_info[j]['label_type'],\n",
    "                        'similarity': similarity_matrix[i][j]\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 1. Get similarity scores\n",
    "similarity_df = analyze_semantic_similarity_bert(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find most similar and distant labels\n",
    "most_similar = similarity_df.nlargest(20, 'similarity')\n",
    "most_distant = similarity_df.nsmallest(20, 'similarity')\n",
    "\n",
    "print(\"Most Similar Labels:\")\n",
    "print(most_similar[['topic', 'label1', 'label2', 'similarity']])\n",
    "\n",
    "print(\"\\nMost Distant Labels:\")\n",
    "print(most_distant[['topic', 'label1', 'label2', 'similarity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb918f2",
   "metadata": {},
   "source": [
    "## Clustering Similar Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method : Clustering Similar Labels\n",
    "def cluster_labels(df):\n",
    "    \"\"\"Cluster labels to find groups of similar concepts\"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Collect all unique labels\n",
    "    all_labels = []\n",
    "    label_metadata = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        for label_col in ['Label_1', 'Label_2', 'Label_3']:\n",
    "            if pd.notna(row[label_col]) and row[label_col] != '':\n",
    "                all_labels.append(row[label_col])\n",
    "                label_metadata.append({\n",
    "                    'label': row[label_col],\n",
    "                    'topic': row['Topic'],\n",
    "                    'annotator': row['Annotator'],\n",
    "                    'label_type': label_col\n",
    "                })\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.encode(all_labels)\n",
    "    \n",
    "    # Cluster using DBSCAN\n",
    "    clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine').fit(embeddings)\n",
    "    \n",
    "    # Add cluster labels to metadata\n",
    "    for i, cluster in enumerate(clustering.labels_):\n",
    "        label_metadata[i]['cluster'] = cluster\n",
    "    \n",
    "    # Create cluster summary\n",
    "    cluster_df = pd.DataFrame(label_metadata)\n",
    "    cluster_summary = cluster_df.groupby('cluster').agg({\n",
    "        'label': lambda x: list(x),\n",
    "        'topic': lambda x: list(set(x)),\n",
    "        'annotator': lambda x: list(set(x))\n",
    "    }).reset_index()\n",
    "    \n",
    "    return cluster_df, cluster_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74676e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Cluster analysis\n",
    "cluster_df, cluster_summary = cluster_labels(summary_df)\n",
    "print(\"\\nLabel Clusters:\")\n",
    "for _, row in cluster_summary.iterrows():\n",
    "    if row['cluster'] != -1:  # -1 is noise in DBSCAN\n",
    "        print(f\"\\nCluster {row['cluster']}:\")\n",
    "        for label in row['label'][:5]:  # Show first 5 labels\n",
    "            print(f\"  - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since you already have similarity_df, let's work with that data\n",
    "\n",
    "# Method 1: Visualize similarity distribution\n",
    "def plot_similarity_distribution(similarity_df):\n",
    "    \"\"\"Plot distribution of similarity scores\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Overall distribution\n",
    "    ax1.hist(similarity_df['similarity'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(similarity_df['similarity'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {similarity_df[\"similarity\"].mean():.3f}')\n",
    "    ax1.set_xlabel('Similarity Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Label Similarities')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Distribution by topic\n",
    "    topic_similarities = similarity_df.groupby('topic')['similarity'].mean().sort_values()\n",
    "    topic_similarities.plot(kind='barh', ax=ax2)\n",
    "    ax2.set_xlabel('Average Similarity')\n",
    "    ax2.set_title('Average Label Similarity by Topic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Method 3: Analyze perfect matches and disagreements\n",
    "def analyze_extreme_cases(similarity_df):\n",
    "    \"\"\"Analyze cases of perfect agreement and strong disagreement\"\"\"\n",
    "    \n",
    "    # Perfect matches (similarity = 1.0)\n",
    "    perfect_matches = similarity_df[similarity_df['similarity'] == 1.0]\n",
    "    \n",
    "    # Group by label to see which labels are used by multiple annotators\n",
    "    label_usage = defaultdict(list)\n",
    "    \n",
    "    for _, row in perfect_matches.iterrows():\n",
    "        label = row['label1']  # Since label1 = label2 for perfect matches\n",
    "        label_usage[label].append({\n",
    "            'topic': row['topic'],\n",
    "            'annotators': sorted([row['annotator1'], row['annotator2']])\n",
    "        })\n",
    "    \n",
    "    print(\"Labels with Perfect Agreement Across Annotators:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for label, usages in sorted(label_usage.items(), \n",
    "                                key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "        topics = set([u['topic'] for u in usages])\n",
    "        print(f\"\\nLabel: '{label}'\")\n",
    "        print(f\"Used in topics: {', '.join(sorted(topics))}\")\n",
    "        print(f\"Number of perfect matches: {len(usages)}\")\n",
    "    \n",
    "    # Analyze disagreements (low similarity)\n",
    "    disagreements = similarity_df[similarity_df['similarity'] < 0.1]\n",
    "    \n",
    "    print(\"\\n\\nTopics with Strongest Disagreements:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    disagreement_by_topic = disagreements.groupby('topic').agg({\n",
    "        'similarity': ['count', 'mean', 'min']\n",
    "    }).round(3)\n",
    "    \n",
    "    disagreement_by_topic.columns = ['count', 'avg_similarity', 'min_similarity']\n",
    "    disagreement_by_topic = disagreement_by_topic.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(disagreement_by_topic.head(10))\n",
    "    \n",
    "    return perfect_matches, disagreements\n",
    "\n",
    "# Method 4: Create similarity matrix for each topic\n",
    "def create_topic_similarity_matrices(similarity_df, topics_to_show=6):\n",
    "    \"\"\"Create similarity matrices for selected topics\"\"\"\n",
    "    \n",
    "    # Get topics with most variation in similarity\n",
    "    topic_stats = similarity_df.groupby('topic')['similarity'].agg(['mean', 'std'])\n",
    "    interesting_topics = topic_stats.sort_values('std', ascending=False).head(topics_to_show).index\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, topic in enumerate(interesting_topics):\n",
    "        topic_data = similarity_df[similarity_df['topic'] == topic]\n",
    "        \n",
    "        # Get unique labels for this topic\n",
    "        labels = sorted(set(topic_data['label1'].unique()) | set(topic_data['label2'].unique()))\n",
    "        \n",
    "        # Create similarity matrix\n",
    "        sim_matrix = pd.DataFrame(0.0, index=labels, columns=labels)\n",
    "        \n",
    "        # Fill diagonal with 1s\n",
    "        for label in labels:\n",
    "            sim_matrix.loc[label, label] = 1.0\n",
    "        \n",
    "        # Fill with similarity values\n",
    "        for _, row in topic_data.iterrows():\n",
    "            sim_matrix.loc[row['label1'], row['label2']] = row['similarity']\n",
    "            sim_matrix.loc[row['label2'], row['label1']] = row['similarity']\n",
    "        \n",
    "        # Truncate labels for display\n",
    "        display_labels = [l[:15] + '...' if len(l) > 15 else l for l in labels]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(sim_matrix, \n",
    "                   xticklabels=display_labels,\n",
    "                   yticklabels=display_labels,\n",
    "                   cmap='RdYlBu_r',\n",
    "                   center=0.5,\n",
    "                   vmin=-0.1, vmax=1,\n",
    "                   ax=axes[idx],\n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        \n",
    "        axes[idx].set_title(f'{topic}\\n(std: {topic_stats.loc[topic, \"std\"]:.3f})', \n",
    "                           fontsize=10)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].tick_params(axis='y', rotation=0)\n",
    "    \n",
    "    plt.suptitle('Similarity Matrices for Topics with Highest Variation', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Method 5: Annotator agreement analysis\n",
    "def analyze_annotator_patterns(similarity_df):\n",
    "    \"\"\"Analyze patterns in annotator agreement\"\"\"\n",
    "    \n",
    "    # Calculate average similarity between each pair of annotators\n",
    "    annotator_pairs = defaultdict(list)\n",
    "    \n",
    "    for _, row in similarity_df.iterrows():\n",
    "        if row['annotator1'] != row['annotator2']:\n",
    "            pair = tuple(sorted([row['annotator1'], row['annotator2']]))\n",
    "            annotator_pairs[pair].append(row['similarity'])\n",
    "    \n",
    "    # Calculate average similarity for each pair\n",
    "    pair_similarities = {}\n",
    "    for pair, sims in annotator_pairs.items():\n",
    "        pair_similarities[pair] = {\n",
    "            'avg_similarity': np.mean(sims),\n",
    "            'std_similarity': np.std(sims),\n",
    "            'count': len(sims)\n",
    "        }\n",
    "    \n",
    "    # Convert to dataframe for easier analysis\n",
    "    pair_df = pd.DataFrame(pair_similarities).T\n",
    "    pair_df.index = pd.MultiIndex.from_tuples(pair_df.index, names=['annotator1', 'annotator2'])\n",
    "    \n",
    "    # Create heatmap of average similarities\n",
    "    annotators = sorted(set(similarity_df['annotator1'].unique()) | \n",
    "                       set(similarity_df['annotator2'].unique()))\n",
    "    \n",
    "    sim_matrix = pd.DataFrame(1.0, index=annotators, columns=annotators)\n",
    "    \n",
    "    for (ann1, ann2), row in pair_df.iterrows():\n",
    "        sim_matrix.loc[ann1, ann2] = row['avg_similarity']\n",
    "        sim_matrix.loc[ann2, ann1] = row['avg_similarity']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(sim_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                vmin=0, vmax=1, square=True)\n",
    "    plt.title('Average Label Similarity Between Annotators')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pair_df\n",
    "\n",
    "# Run the analyses\n",
    "print(\"Analyzing your similarity data...\")\n",
    "\n",
    "# 1. Plot similarity distribution\n",
    "plot_similarity_distribution(similarity_df)\n",
    "\n",
    "\n",
    "# 3. Analyze extreme cases\n",
    "print(\"\\nAnalyzing extreme cases...\")\n",
    "perfect_matches, disagreements = analyze_extreme_cases(similarity_df)\n",
    "\n",
    "# 4. Create similarity matrices for interesting topics\n",
    "print(\"\\nCreating topic similarity matrices...\")\n",
    "create_topic_similarity_matrices(similarity_df)\n",
    "\n",
    "# 5. Analyze annotator patterns\n",
    "print(\"\\nAnalyzing annotator agreement patterns...\")\n",
    "pair_df = analyze_annotator_patterns(similarity_df)\n",
    "print(\"\\nMost agreeable annotator pairs:\")\n",
    "print(pair_df.sort_values('avg_similarity', ascending=False).head())\n",
    "print(\"\\nLeast agreeable annotator pairs:\")\n",
    "print(pair_df.sort_values('avg_similarity', ascending=True).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa75192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_topic_networks(summary_df, similarity_threshold=0.5, difference_threshold=0.2):\n",
    "    \"\"\"Create network visualization for each topic showing similar and different labels\"\"\"\n",
    "    \n",
    "    # Get unique topics\n",
    "    topics = sorted(summary_df['Topic'].unique())\n",
    "    \n",
    "    # Process each topic\n",
    "    for topic in topics:\n",
    "        topic_data = summary_df[summary_df['Topic'] == topic]\n",
    "        \n",
    "        # Collect all labels with metadata\n",
    "        labels_info = []\n",
    "        label_texts = []\n",
    "        \n",
    "        for _, row in topic_data.iterrows():\n",
    "            for i, label_col in enumerate(['Label_1', 'Label_2', 'Label_3']):\n",
    "                if pd.notna(row[label_col]) and row[label_col] != '':\n",
    "                    label_info = {\n",
    "                        'text': row[label_col],\n",
    "                        'annotator': row['Annotator'],\n",
    "                        'label_type': f'L{i+1}',\n",
    "                        'full_label': f\"{row['Annotator'][:4]}_{i+1}: {row[label_col][:30]}...\"\n",
    "                        if len(row[label_col]) > 30 else f\"{row['Annotator'][:4]}_{i+1}: {row[label_col]}\"\n",
    "                    }\n",
    "                    labels_info.append(label_info)\n",
    "                    label_texts.append(row[label_col])\n",
    "        \n",
    "        if len(label_texts) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarities\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(label_texts)\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Create network\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for i, label_info in enumerate(labels_info):\n",
    "            G.add_node(i, **label_info)\n",
    "        \n",
    "        # Add edges based on similarity\n",
    "        similar_edges = []\n",
    "        different_edges = []\n",
    "        \n",
    "        for i in range(len(labels_info)):\n",
    "            for j in range(i+1, len(labels_info)):\n",
    "                sim = similarity_matrix[i][j]\n",
    "                \n",
    "                if sim > similarity_threshold:\n",
    "                    G.add_edge(i, j, weight=sim, edge_type='similar')\n",
    "                    similar_edges.append((i, j))\n",
    "                elif sim < difference_threshold:\n",
    "                    G.add_edge(i, j, weight=sim, edge_type='different')\n",
    "                    different_edges.append((i, j))\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Use spring layout with high k value to spread nodes\n",
    "        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "        \n",
    "        # Draw nodes with colors based on annotator\n",
    "        annotators = list(set([info['annotator'] for info in labels_info]))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(annotators)))\n",
    "        annotator_colors = dict(zip(annotators, colors))\n",
    "        \n",
    "        node_colors = [annotator_colors[labels_info[node]['annotator']] for node in G.nodes()]\n",
    "        \n",
    "        # Draw the network\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=800, alpha=0.8)\n",
    "        \n",
    "        # Draw similar edges (thick, green)\n",
    "        if similar_edges:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=similar_edges,\n",
    "                                  width=3, alpha=0.6, edge_color='green',\n",
    "                                  style='solid')\n",
    "        \n",
    "        # Draw different edges (thin, red, dashed)\n",
    "        if different_edges:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=different_edges,\n",
    "                                  width=1, alpha=0.4, edge_color='red',\n",
    "                                  style='dashed')\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {i: labels_info[i]['full_label'] for i in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # Add title and legend\n",
    "        plt.title(f'Label Network for {topic}\\n' + \n",
    "                 f'Green solid lines: Similar (>{similarity_threshold}), ' +\n",
    "                 f'Red dashed lines: Different (<{difference_threshold})',\n",
    "                 fontsize=14, pad=20)\n",
    "        \n",
    "        # Create legend for annotators\n",
    "        legend_elements = []\n",
    "        for annotator, color in annotator_colors.items():\n",
    "            legend_elements.append(plt.scatter([], [], c=[color], s=100, \n",
    "                                             label=annotator, alpha=0.8))\n",
    "        \n",
    "        plt.legend(handles=legend_elements, title='Annotators', \n",
    "                  loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics for this topic\n",
    "        print(f\"\\n{topic} Statistics:\")\n",
    "        print(f\"  Total labels: {len(labels_info)}\")\n",
    "        print(f\"  Similar pairs: {len(similar_edges)}\")\n",
    "        print(f\"  Different pairs: {len(different_edges)}\")\n",
    "        \n",
    "        # Find and print the most similar and different pairs\n",
    "        if similar_edges or different_edges:\n",
    "            print(f\"\\n  Most similar labels:\")\n",
    "            similar_pairs = [(i, j, similarity_matrix[i][j]) for i, j in similar_edges]\n",
    "            similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "            for i, j, sim in similar_pairs[:3]:\n",
    "                print(f\"    - '{labels_info[i]['text'][:40]}...' vs '{labels_info[j]['text'][:40]}...'\")\n",
    "                print(f\"      ({labels_info[i]['annotator']} vs {labels_info[j]['annotator']}, similarity: {sim:.3f})\")\n",
    "            \n",
    "            if different_edges:\n",
    "                print(f\"\\n  Most different labels:\")\n",
    "                different_pairs = [(i, j, similarity_matrix[i][j]) for i, j in different_edges]\n",
    "                different_pairs.sort(key=lambda x: x[2])\n",
    "                for i, j, sim in different_pairs[:3]:\n",
    "                    print(f\"    - '{labels_info[i]['text'][:40]}...' vs '{labels_info[j]['text'][:40]}...'\")\n",
    "                    print(f\"      ({labels_info[i]['annotator']} vs {labels_info[j]['annotator']}, similarity: {sim:.3f})\")\n",
    "\n",
    "def create_topic_similarity_matrices(summary_df, topics_per_page=6):\n",
    "    \"\"\"Create similarity matrices showing all pairwise similarities for each topic\"\"\"\n",
    "    \n",
    "    topics = sorted(summary_df['Topic'].unique())\n",
    "    n_pages = (len(topics) + topics_per_page - 1) // topics_per_page\n",
    "    \n",
    "    for page in range(n_pages):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        start_idx = page * topics_per_page\n",
    "        end_idx = min(start_idx + topics_per_page, len(topics))\n",
    "        \n",
    "        for idx, topic_idx in enumerate(range(start_idx, end_idx)):\n",
    "            topic = topics[topic_idx]\n",
    "            topic_data = summary_df[summary_df['Topic'] == topic]\n",
    "            \n",
    "            # Collect labels\n",
    "            labels_info = []\n",
    "            label_texts = []\n",
    "            display_labels = []\n",
    "            \n",
    "            for _, row in topic_data.iterrows():\n",
    "                for i, label_col in enumerate(['Label_1', 'Label_2', 'Label_3']):\n",
    "                    if pd.notna(row[label_col]) and row[label_col] != '':\n",
    "                        label_texts.append(row[label_col])\n",
    "                        display_labels.append(f\"{row['Annotator'][:3]}_L{i+1}\")\n",
    "            \n",
    "            if len(label_texts) > 1:\n",
    "                # Calculate similarities\n",
    "                vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "                try:\n",
    "                    tfidf_matrix = vectorizer.fit_transform(label_texts)\n",
    "                    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "                    \n",
    "                    # Create heatmap\n",
    "                    im = axes[idx].imshow(similarity_matrix, cmap='RdYlGn', \n",
    "                                         vmin=-0.1, vmax=1, aspect='auto')\n",
    "                    \n",
    "                    # Add colorbar\n",
    "                    cbar = plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "                    cbar.ax.tick_params(labelsize=8)\n",
    "                    \n",
    "                    # Set ticks and labels\n",
    "                    axes[idx].set_xticks(range(len(display_labels)))\n",
    "                    axes[idx].set_yticks(range(len(display_labels)))\n",
    "                    axes[idx].set_xticklabels(display_labels, rotation=45, ha='right', fontsize=8)\n",
    "                    axes[idx].set_yticklabels(display_labels, fontsize=8)\n",
    "                    \n",
    "                    # Add grid\n",
    "                    axes[idx].set_xticks(np.arange(len(display_labels)+1)-0.5, minor=True)\n",
    "                    axes[idx].set_yticks(np.arange(len(display_labels)+1)-0.5, minor=True)\n",
    "                    axes[idx].grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.5)\n",
    "                    \n",
    "                    # Add text annotations for similarity values\n",
    "                    for i in range(len(similarity_matrix)):\n",
    "                        for j in range(len(similarity_matrix)):\n",
    "                            text = axes[idx].text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                                                ha=\"center\", va=\"center\", \n",
    "                                                color=\"white\" if similarity_matrix[i, j] < 0.5 else \"black\",\n",
    "                                                fontsize=7)\n",
    "                    \n",
    "                    axes[idx].set_title(f'{topic}', fontsize=10, pad=10)\n",
    "                except:\n",
    "                    axes[idx].axis('off')\n",
    "            else:\n",
    "                axes[idx].axis('off')\n",
    "        \n",
    "        # Turn off unused axes\n",
    "        for i in range(end_idx - start_idx, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Label Similarity Matrices - Page {page+1}/{n_pages}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def create_summary_network(summary_df, min_similarity=0.7):\n",
    "    \"\"\"Create a summary network showing all highly similar labels across all topics\"\"\"\n",
    "    \n",
    "    # Collect all unique labels with their topic and annotator info\n",
    "    all_labels = {}\n",
    "    label_id = 0\n",
    "    \n",
    "    for _, row in summary_df.iterrows():\n",
    "        topic = row['Topic']\n",
    "        annotator = row['Annotator']\n",
    "        \n",
    "        for i, label_col in enumerate(['Label_1', 'Label_2', 'Label_3']):\n",
    "            if pd.notna(row[label_col]) and row[label_col] != '':\n",
    "                label_key = (row[label_col], topic, annotator)\n",
    "                if label_key not in all_labels:\n",
    "                    all_labels[label_key] = {\n",
    "                        'id': label_id,\n",
    "                        'text': row[label_col],\n",
    "                        'topic': topic,\n",
    "                        'annotator': annotator,\n",
    "                        'label_type': f'L{i+1}'\n",
    "                    }\n",
    "                    label_id += 1\n",
    "    \n",
    "    # Get unique label texts\n",
    "    unique_texts = list(set(info['text'] for info in all_labels.values()))\n",
    "    \n",
    "    if len(unique_texts) < 2:\n",
    "        print(\"Not enough unique labels for summary network\")\n",
    "        return\n",
    "    \n",
    "    # Calculate similarities between unique texts\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(unique_texts)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Create network of highly similar labels\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for each unique label text\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        # Find all instances of this label\n",
    "        instances = [info for info in all_labels.values() if info['text'] == text]\n",
    "        topics = list(set(inst['topic'] for inst in instances))\n",
    "        annotators = list(set(inst['annotator'] for inst in instances))\n",
    "        \n",
    "        G.add_node(i, text=text[:50] + '...' if len(text) > 50 else text,\n",
    "                  topics=topics, annotators=annotators,\n",
    "                  count=len(instances))\n",
    "    \n",
    "    # Add edges for high similarity\n",
    "    for i in range(len(unique_texts)):\n",
    "        for j in range(i+1, len(unique_texts)):\n",
    "            if similarity_matrix[i][j] >= min_similarity:\n",
    "                G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=4, iterations=50)\n",
    "    \n",
    "    # Node sizes based on frequency\n",
    "    node_sizes = [G.nodes[node]['count'] * 300 for node in G.nodes()]\n",
    "    \n",
    "    # Node colors based on number of topics\n",
    "    node_colors = [len(G.nodes[node]['topics']) for node in G.nodes()]\n",
    "    \n",
    "    # Draw network\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                          node_color=node_colors, cmap='viridis',\n",
    "                          alpha=0.7)\n",
    "    \n",
    "    # Draw edges with varying thickness\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] * 5 for u, v in edges]\n",
    "    nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {node: G.nodes[node]['text'] for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(f'Summary Network of Similar Labels (Similarity >= {min_similarity})\\n' +\n",
    "             'Node size = frequency, Node color = number of topics',\n",
    "             fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                               norm=plt.Normalize(vmin=min(node_colors), \n",
    "                                                vmax=max(node_colors)))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca(), fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Number of Topics', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualizations\n",
    "print(\"Creating network visualizations for each topic...\")\n",
    "create_topic_networks(summary_df, similarity_threshold=0.5, difference_threshold=0.2)\n",
    "\n",
    "print(\"\\n\\nCreating similarity matrices for all topics...\")\n",
    "create_topic_similarity_matrices(summary_df)\n",
    "\n",
    "print(\"\\n\\nCreating summary network of all similar labels...\")\n",
    "create_summary_network(summary_df, min_similarity=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
